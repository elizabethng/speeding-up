---
title: "Speeding things up"
subtitle: "FSH 507 Fall 2019"
author: "John Trochta & Dan Ovando"
institute: "SAFS, UW"
date: " `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
```

# BIG problems...
---
```{r}
knitr::include_graphics("images/Levi et al 2016.png")
```
.footnote[From Levi et al. 2016 [doi:10.5063/F1Z899CZ](https://knb.ecoinformatics.org/view/doi:10.5063/F1Z899CZ)]
???
_here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America_
_There are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization_

---
class: center, middle
# SLOW problems...

---
class: center, middle
<video autosize: true controls>
  <source src="images/Model_run.mov">
</video>
???
_Gotta get this video working or find some other visual. Basically bring up running thousands of simulations_

---
class: left, top
# In this class:

### How does parallelization occur?

### Parallelizing in R with focus on `doParallel`

### Other tricks for speeding up: `profvis`, `Rcpp`, & Google Virtual Machines

### At the very least, set up parallelization on your own machine & introduce resources for further efficiency improvements

---
class: left, top
# Acknowledgments:

Materials for this lecture drawn from:

[Jonathan Dursi's Beyond Single-Core R](https://ljdursi.github.io/beyond-single-core-R/#/)

[Matt Jones' Quick Intro to Parallel Computing in R](Quickhttps://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html)

[`doParallel` vignette](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf)

---
class: left, top

# Why parallelism? Boils down to 3 reasons:

1. Code too SLOW

2. Problem too BIG

3. Tasks too MANY

#### Modern processor architectures accomodate multiple **cores** and **processors**

```{r}
knitr::include_graphics("https://media.giphy.com/media/7XsFGzfP6WmC4/giphy.gif")
```
???
_Splitting the problem up onto multiple cores, or rather more computers will grant access to enough memory to run effectively_
_One task takes reasonable amount of time, but I have to run thousands_
_Splitting tasks may improve efficiency_

---
class: left, top
# The Hardware

.pull-left[
- Node: A single motherboard, with possibly multiple sockets

- Processor/Socket: the silicon containing likely multiple cores

- Core: the unit of computation; often has hardware support for

- Pseudo-cores: can appear to the OS as multiple cores but share much functionality between other pseudo-cores on the same core
]

.pull-right[
```{r out.width = '150%'}
knitr::include_graphics("images/sockets-cores.png")
```
]

---
class: left, top
# The Hardware

.pull-left[
What this means for you?

Say your computer has a quad-core processor, then you can execute 4 computations in parallel


My 2018 MacBook Pro
```{r out.width = '100%'}
knitr::include_graphics("images/my-macbook-cpus.jpg")
```
6 CPUs x 2 cores = 12 cores for computation
]

.pull-right[
```{r out.width = '150%'}
knitr::include_graphics("images/sockets-cores.png")
```
]

---
class: left, top
# Processes and threads:

.pull-left[
- Process: Data and code in memory

- One or more **threads** of execution within a process

- Threads in the same process can see most of the same memory

- Processes generally cannot peer into another processes memory
]
.pull-right[
```{r}
knitr::include_graphics("images/process-threads.png")
```
]
???
_Can use multiple cores, multiple processes, or both_

---
class: left, top
# Rationale: Independant Computations
.pull-left[
- For more cores to help, there has to be something for them to do.

- Find largely independent tasks to occupy them.

- Example (for modelers): simulation study

- No individual task runs any faster with more processors, but the workload as a whole can.
]
.pull-right[
```{r}
knitr::include_graphics("images/paramstudy.png")
```
]

---
class: left, top
## Rationale: Split, analyze (separately), & combine
.pull-left[
- Parallels line of thinking in tidyverse

- Can split data or tasks up between computing elements

- Largely depends on data & analysis types
]
.pull-right[
```{r out.width= '90%'}
knitr::include_graphics("images/split-apply-combine.png")
```
]

---
class: left, top
## Rationale: Split, analyze (separately), & combine
- Costs to "splitting" & "combining"

- These must be done in serial at initial & final stages

- When scaling from single to multiple machines, even more costly

- Basis for Amdal's Law:
$$
T \approx \left ( f + \frac{1 - f}{P} \right )
$$
???
_It’s not as simple as it may seem._
_While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency._
_For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth._
_Plus, new processes and/or threads need to be created by the operating system, which also takes time._
_This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power._
_For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!_
_In addition, not all of a task can be parallelized_
---
class: center, top
# Rationale: Amdal's Law
$$
T \approx \left ( f + \frac{1 - f}{P} \right )
$$
```{r echo=FALSE, out.width = '60%'}
p <- 1:12
amdall.efficiency <- function(f,p) (1./p) / (f + (1.-f)/p)
serial.fracs <- seq(0.,.75,by=.15)
eff <- matrix(nrow=length(serial.fracs), ncol=length(p))
for (i in 1:length(serial.fracs)) {
  eff[i,] <- sapply(p, function(np) amdall.efficiency(serial.fracs[i], np))
}
matplot(t(eff), type = c("b"), pch=16, xlab="Number of Processors", ylab="Efficiency", col=1:6)
legend("topright", legend = serial.fracs,  pch=16, col=1:6, title="Proportion parallelized")
```

---
class: center, middle
# Parallelization in R

---
class: left, top
# R Packages for parallelization

-`multicore`: use all cores on single processor, excludes Windows

-`snow`: use any group of processors

-`parallel`: basically merges the above 2, part of core R
???
_Many packages which use parallelism use one of these two, so worth understanding_
_Both create new processes (not threads) to run on different processors; but in importantly different ways_

---
class: left, top
# Forking in `multicore`
.pull-left[
- `multicore` starts new processes by forking

- New processes see copy of original data, then fork new ones

- Memory is NOT shared

- Forking & rejoining means overhead
]
.pull-right[
```{r out.width= '90%'}
knitr::include_graphics("images/fork-sm.png")
```
]
???
_Not shared memory; changes in one process will not be reflected in others_
_the copy of memory isn't made unless it has to be, and it doesn't have to be until one process or the other writes to the memory_
_forking the processes and waiting for them to rejoin itself takes some time._
_This overhead means that we want to launch jobs that take a significant length of time to run - much longer than the overhead_

---
class: left, top
# Spawning in `snow`
.pull-left[
- `snow` creates entirely new processes

- Need to explicitly copy data & functions

- Can be done remotely,
]
.pull-right[
```{r out.width= '90%'}
knitr::include_graphics("images/spawn-sm.png")
```
]
???
_A downside is that you need to explicitly copy over any needed data, functions._
_But the upsides are that spawning a new process can be done on a remote machine, not just current machine. So you can in principle use entire clusters._
_the flipside of the downside: new processes don't have any unneeded data, less total memory footprint._

---
class: left, top
# `multicore` and `snow`
- Simplest use of `multicore` is through

  1. `mcparallel()` which forks a task ('task parallelism')
  
  2. `mccollect()` which gets result
  
  3. `mcapply()` the multicore equivalent of `lapply` ('data parallelism')

- Further details & examples at [Beyond Single-Core R](https://ljdursi.github.io/beyond-single-core-R/#/)

- Turn focus now to `doParallel`

---
class: left, top
# `doParallel` package

- "Parallel backend" for the `foreach` package

- Acts as interface between `foreach` & `parallel`

- Designed to go from serial to various forms of parallel

- Based on for loops for iteration

???
_The foreach package is based on another style of iterating through data - a for loop - and is designed so that one can go from serial to several forms of parallel relatively easily. There are then a number of tools one can use in the library to improve performance._

---
class: left, top
# Working serially with `foreach`

The standard R for loop looks like this:
```{r echo = TRUE}
for (i in 1:3) print(sqrt(i))
```

The foreach operator looks similar, but returns a list of the iterations:
```{r echo = TRUE}
library(foreach)
foreach (i=1:3) %do% sqrt(i)
```

---
class: left, top
# Working serially with `foreach`

```{r eval=FALSE, echo=TRUE}
library(foreach)
foreach (i=1:3) %do% sqrt(i)
```
`foreach` creates an object & `%do%` operates on the code (here just one statement,
but it can be multiple lines between braces, as with a for loop) and the `foreach` object.

---
class: left, top
# `foreach`  &  `doParallel`

Foreach works with variety of backends to distribute computation - `doParallel`, which allows snow- and
multicore-style parallelism   

Switching the above loop to parallel just requires registering a backend and using `%dopar%` rather than `%do%`:
```{r}
library(parallel)
library(iterators)
```
```{r echo=TRUE}
library(doParallel)
registerDoParallel(3)  # use multicore-style forking
foreach (i=1:3) %dopar% sqrt(i)
stopImplicitCluster()
```

---
class: left, top
# Combining results

Default is to return a list, but there are numerous ways to combine results:
```{r echo=TRUE}
foreach (i=1:3, .combine=c) %do% sqrt(i)
foreach (i=1:3, .combine=cbind) %do% sqrt(i)
foreach (i=1:3, .combine="+") %do% sqrt(i)
foreach (i=1:3, .multicombine=TRUE, .combine="sum") %do% sqrt(i)
```

---
class: left, top
# Combining results

Most of these are self explanatory.

`multicombine` is worth mentioning: by default, `foreach` will combine
each new item into the final result one-at-a-time.

If `.multicombine=TRUE`, then you are saying that you're passing a function
which will do the right thing even if foreach gives it a whole wack of new results as a list or vector -
*e.g.*, a whole chunk at a time.

---
class: left, top
# Nesting `foreach`

- There's one more operator: `%:%`.  This lets you compose or nest foreach objects:
```{r echo=TRUE}
foreach (i=1:3, .combine="c") %:% 
  foreach (j=1:3, .combine="c") %do% {
    i*j
  }
```

---
class: left, top
# Other tips with `foreach` & `doParallel`

Can suffer same inefficiencies as a `for` loop if looping over MANY iterations

for example:
```{r eval=FALSE,echo=TRUE}
  ans <- foreach (i=1:2000, .combine="+") %do% {
    ...
  }
```
creates a vector that takes memory

---
class: left, top
# Other tips with `foreach` & `doParallel`

## Possible solution:

Foreach has _iterators_ (e.g. `icount`) that can iterate through an object without creating something the size of the object.

for example:
```{r eval=FALSE,echo=TRUE}
  ans <- foreach (i=icount(2000), .combine="+") %do% {
    ...
  }
```

---
class: left, top
# Other tips with `foreach` & `doParallel`

## Better solution:
Avoid the intermediate index altogether because we don't care about $i$, we care about the data. So we either implicitly iterate:
```{r eval=FALSE,echo=TRUE}
  ans <- foreach (i=DATA$VARIABLE_1(2000), .combine="+") %do% {
    ...
  }
```

---
class: left, top
# Other tips with `foreach` & `doParallel`

## Better solution:
OR explicitly iterate by 'chunking:
```{r eval=FALSE,echo=TRUE}
  ans <- foreach (i=iter(DATA$VARIABLE_1(2000),chunksize=500),
                  .combine="+") %do% {}
```
OR even by row, column, etc if your object is a matrix or data frame
```{r eval=FALSE,echo=TRUE}
  ans <- foreach (i=iter(DATA_MATRIX,by="row"), .combine="+") %do% {}
```
This works in parallel too.

---
class: left, top
# Other tips with `foreach` & `doParallel`

Chunking is just one of several ways `foreach` adjusts the iteration scheduling

For instance, you can use options from `multicore` (if using a `multicore` cluster)
```{r eval=FALSE, echo=TRUE}
foreach( ..., .options.multicore=list(preschedule=FALSE,set.seed=TRUE))
```

Another is if you don't care about the order that the results come back in, use:
```{r eval=FALSE, echo=TRUE}
foreach( ..., .inorder=FALSE)
```
to allow flexibility in sending out tasks (otherwise, first result is ALWAYS from first iteration, etc.)

---
class: left, top
# Summary of `foreach`

Since it is a wrapper for `multicore` & `snow`, it inherits pros & cons of each

I prefer to use it because:

1. I (perhaps naively) rely on `for` loops already making the transition easy

2. You don't have to commit to `multicore` or `snow`; just switch to either in `doParallel` by registering the backend

3. You can incrementally improve code performance

---
class: left, top
# Best practices

## Don't reinvent wheels

There's no one-size-fits-all-analysis approach to taking advantage of multiple processors or nodes

Check to see if there already exists packages for doing your analysis in parallel.

For a complete list checkout
[http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html)

.footnote[from [Beyond Single-Core R by Jonathan Dursi](https://ljdursi.github.io/beyond-single-core-R/#/)]

---
class: left, top
# Best practices

## Big chunks better than little chunks

Makes for much more efficient coding (recall Amdahl's Law)

_E.g._ running each cross-validation fold on a different processor VS running them sequentially but with parts of the training method parallelized.

.footnote[from [Beyond Single-Core R by Jonathan Dursi](https://ljdursi.github.io/beyond-single-core-R/#/)]

---
class: left, top
# Best practices

## Parallelism boosts computational power

For throwing more cores at the problem to help, the calculation has to have been limited by compute power
(some analyses, such as **I/O bound computations**-completion time determined by amount of time spent reading/writing from disk-or **network-bound**-timed determined by transfer-,can actually perform worse)

.footnote[from [Beyond Single-Core R by Jonathan Dursi](https://ljdursi.github.io/beyond-single-core-R/#/)]

---
class: left, top
# Best practices

## One task per core, no more

CPU-bound (time to completion determined by speed of central processor) computations slow down if something else is sharing the processor with them.

Except for debugging(?), best not to run `doParallel` with more cores than you
have on your computer _(never actually tried)_.

.footnote[from [Beyond Single-Core R by Jonathan Dursi](https://ljdursi.github.io/beyond-single-core-R/#/)]

---
class: left, top
# Best practices

## Don't trip over your own feet

Because you only want one task for core, be careful about accidentally using parallelism (mostly with higher-level packages)

_e.g._ If you're 
- doing four tasks at once with multicore on your laptop, and
- Each of those tasks calls a routine with "baked-in" parallelism to launch 4 threads

Now you're competing with yourself for your own laptop's cores.

.footnote[from [Beyond Single-Core R by Jonathan Dursi](https://ljdursi.github.io/beyond-single-core-R/#/)]

---
class: center, middle
# Other tricks for speeding up

---
class: left, top
# `profvis` package

interactive graphical interface for `Rprof`, R's built-in profiling tool

[`profvis` tutorial](https://rstudio.github.io/profvis/index.html)

---
class: left, top
# `Rcpp` package

- Parallelization can only go so far

--

- Bottlenecks can be fixed by using lower level languages to rewrite key functions

--

- However, writing C or Fortrain for use in R is a pain on its own (even if you are fluent)

--

- Enter `Rcpp`

---
class: left, top
# `Rcpp` package

Typical bottlenecks:

- Loops where each iteration depends on previous one (i.e. cannot be vectorized)

- Functions called many, many times (talking millions); overhead on C++ functions much lower

- Advanced data structures and algorithms that R doesn't provide

---
class: left, top
# `Rcpp` package

Installing & loading the package isn't enough
```{r eval=FALSE, echo=TRUE}
library(Rcpp)
```

You will all need to install a C++ compiler. To do so:

- On Windows, install Rtools.

- On Mac, install Xcode from the app store.

- On Linux, `sudo apt-get install r-base-dev` or similar.

???
I do not evaluate any of the Rcpp code below to avoid errors on others' machines

---
class: left, top
# the `cppFunction()`

Allows you to write C++ functions in R, e.g.:

```{r eval=FALSE, echo=TRUE}
cppFunction('int add(int x, int y, int z) {
  int sum = x + y + z;
  return sum;
}')
```

`Rcpp` compiles this & constructs an R function connecting to the compiled function:

```{r eval=FALSE, echo=TRUE}
add(1, 2, 3)
```

???
When you run this code, Rcpp will compile the C++ code and construct an R function that connects to the compiled C++ function. There’s a lot going on underneath the hood but Rcpp takes care of all the details so you don’t need to worry about them.

---
class: left, top
# the `sourceCpp()`

`cppFunction()` simplifies presentation, but not typically useful

Use `sourceCpp` to source stand-alone C++ file into R for use

One thing to note is that the C++ file should prefixed with:
```{r eval=FALSE, echo=TRUE}
// [[Rcpp::export]]
```

---
class: left, top
# the `sourceCpp()`

`cppFunction()` simplifies presentation, but not typically useful

Use `sourceCpp` to source stand-alone C++ file into R for use:
```{r eval=FALSE, echo=TRUE}
sourceCpp("MyFunction.cpp")
```

---
class: left, top
# Example C++ file - calculating the mean
```{r eval=FALSE, echo=TRUE}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double meanC(NumericVector x) {
  int n = x.size();
  double total = 0;

  for(int i = 0; i < n; ++i) {
    total += x[i];
  }
  return total / n;
}
```

One thing to note is that the C++ file should prefixed with:
```{r eval=FALSE, echo=TRUE}
// [[Rcpp::export]]
```

---
class: left, top
# Example C++ file - calculating the mean

R code can be embedded in special comment blocks - good for running test code

The following implements `meanC` and compares to the built-in `mean()`
```{r eval=FALSE, echo=TRUE}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double meanC(NumericVector x) {
  int n = x.size();
  double total = 0;
  for(int i = 0; i < n; ++i) {total += x[i];}
  return total / n;
}
/*** R
x <- runif(1e5)
bench::mark(mean(x),meanC(x))
*/
```

---
class: left, top
# `Rcpp` and C++

Check out [Chapter 25 in Advanced R](https://adv-r.hadley.nz/rcpp.html#sourceCpp) for more on `Rcpp`

There he mentions some good references to learn C++ for free:

http://www.learncpp.com/

https://en.cppreference.com/w/cpp

---
class: left, top
# Yet to cover

- Excercises
- Intro to Cloud services (broad overview & re-direct to UW resources)
- cluster of computer/psocket
- more detailed package setup
- flock
