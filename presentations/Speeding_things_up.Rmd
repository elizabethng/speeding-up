---
title: "Speeding things up"
subtitle: "FSH 507 Fall 2019"
author: "John Trochta & Dan Ovando"
institute: "SAFS, UW"
date: " `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
```

# BIG problems...
---
```{r}
knitr::include_graphics("images/Levi et al 2016.png")
```
.footnote[From Levi et al. 2016 [doi:10.5063/F1Z899CZ](https://knb.ecoinformatics.org/view/doi:10.5063/F1Z899CZ)]
???
_here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America_
_There are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization_

---
class: center, middle
# SLOW problems...

---
class: center, middle
<video autosize: true controls>
  <source src="images/Model_run.mov">
</video>
???
_Gotta get this video working or find some other visual. Basically bring up running thousands of simulations_

---
.pull-left[
# In this class:

What is parallelization?

How does parallelization occur?

Parallelizing in R with focus on `doParallel`

Other tricks: `profvis`, `Rcpp`, & Google Virtual Machines
]

--

.pull-right[
# By the end of class:

Run code in parallel using `foreach`


]

---
class: left, top
# Key terms

.pull-left[
- Node: A single motherboard, with possibly multiple sockets

- Processor/Socket: the silicon containing likely multiple cores

- Core: the unit of computation; often has hardware support for

- Pseudo-cores: can appear to the OS as multiple cores but share much functionality between other pseudo-cores on the same core
]

.pull-right[
```{r out.width = '150%'}
knitr::include_graphics("images/sockets-cores.png")
```
]
.footnote[Materials drawn from [Beyond Single-Core R](https://ljdursi.github.io/beyond-single-core-R/#/)]

???
_Can use multiple cores, multiple processes, or both_
---
class: left, top
# Processes and threads:

.pull-left[
- Process: Data and code in memory

- One or more **threads** of execution within a process

- Threads in the same process can see most of the same memory

- Processes generally cannot peer into another processes memory
]
.pull-right[
```{r}
knitr::include_graphics("images/process-threads.png")
```
]

---
class: center, top

# Why parallelism? Boils down to 3 reasons:

##  1. Code too SLOW

##  2. Problem too BIG

##  3. Tasks too MANY

```{r}
knitr::include_graphics("https://media.giphy.com/media/7XsFGzfP6WmC4/giphy.gif")
```
???
_Splitting the problem up onto multiple cores, or rather more computers will grant access to enough memory to run effectively_
_One task takes reasonable amount of time, but I have to run thousands_
_Splitting tasks may improve efficiency_

---
class: left, top
# Rationale: Independant Computations
.pull-left[
- For more cores to help, there has to be something for them to do.

- Find largely independent tasks to occupy them.

- Example (for modelers): simulation study

- No individual task runs any faster with more processors, but the workload as a whole can.
]
.pull-right[
```{r}
knitr::include_graphics("images/paramstudy.png")
```
]

---
class: left, top
## Rationale: Split, analyze (separately), & combine
.pull-left[
- Parallels line of thinking in tidyverse

- Can split data or tasks up between computing elements

- Largely depends on data & analysis types
]
.pull-right[
```{r out.width= '90%'}
knitr::include_graphics("images/split-apply-combine.png")
```
]

---
class: left, top
## Rationale: Split, analyze (separately), & combine
- Costs to "splitting" & "combining"

- These must be done in serial at initial & final stages

- When scaling from single to multiple machines, even more costly

- Basis for Amdal's Law:
$$
T \approx \left ( f + \frac{1 - f}{P} \right )
$$
---
class: center, top
# Rationale: Amdal's Law
$$
T \approx \left ( f + \frac{1 - f}{P} \right )
$$
```{r echo=FALSE, out.width = '60%'}
p <- 1:12
amdall.efficiency <- function(f,p) (1./p) / (f + (1.-f)/p)
serial.fracs <- seq(0.,.75,by=.15)
eff <- matrix(nrow=length(serial.fracs), ncol=length(p))
for (i in 1:length(serial.fracs)) {
  eff[i,] <- sapply(p, function(np) amdall.efficiency(serial.fracs[i], np))
}
matplot(t(eff), type = c("b"), pch=16, xlab="Number of Processors", ylab="Efficiency", col=1:6)
legend("topright", legend = serial.fracs,  pch=16, col=1:6, title="Proportion parallelized")
```

---
class: center, middle
# Parallelization in R

---
class: left, top
# R Packages for parallelization

-`multicore`: use all cores on single processor, excludes Windows

-`snow`: use any group of processors

-`parallel`: basically merges the above 2, part of core R
???
_Many packages which use parallelism use one of these two, so worth understanding_
_Both create new processes (not threads) to run on different processors; but in importantly different ways_

---
class: left, top
# Forking in `multicore`
.pull-left[
- `multicore` starts new processes by forking

- New processes see copy of original data, then fork new ones

- Memory is NOT shared

- Forking & rejoining means overhead
]
.pull-right[
```{r out.width= '90%'}
knitr::include_graphics("images/fork-sm.png")
```
]
???
_Not shared memory; changes in one process will not be reflected in others_
_the copy of memory isn't made unless it has to be, and it doesn't have to be until one process or the other writes to the memory_
_forking the processes and waiting for them to rejoin itself takes some time._
_This overhead means that we want to launch jobs that take a significant length of time to run - much longer than the overhead_

---
class: left, top
# Spawning in `snow`
.pull-left[
- `multicore` creates entirely new processes

- Need to explicitly copy data & functions

- Can be done remotely,
]
.pull-right[
```{r out.width= '90%'}
knitr::include_graphics("images/spawn-sm.png")
```
]
???
_A downside is that you need to explicitly copy over any needed data, functions._
_But the upsides are that spawning a new process can be done on a remote machine, not just current machine. So you can in principle use entire clusters._
_the flipside of the downside: new processes don't have any unneeded data, less total memory footprint._

---
class: left, top
# `multicore` and `snow`
- Simplest use of `multicore` is through
  1. `mcparallel()` which forks a task ('task parallelism')
  2. `mccollect()` which gets result
  3. `mcapply()` the multicore equivalent of `lapply` ('data parallelism')

- Further details & examples at [Beyond Single-Core R](https://ljdursi.github.io/beyond-single-core-R/#/)]

- Turn focus now to `doParallel`

---
class: left, top
# `doParallel` package

- "Parallel backend" for the `foreach` package

- Acts as interface between `foreach` & `parallel`

- Designed to go from serial to various forms of parallel

- Based on for loops for iteration

???
_The foreach package is based on another style of iterating through data - a for loop - and is designed so that one can go from serial to several forms of parallel relatively easily. There are then a number of tools one can use in the library to improve performance._

---
class: left, top
# Working serially with `foreach`

The standard R for loop looks like this:
```{r echo = TRUE}
for (i in 1:3) print(sqrt(i))
```

The foreach operator looks similar, but returns a list of the iterations:
```{r echo = TRUE}
library(foreach)
foreach (i=1:3) %do% sqrt(i)
```

---
class: left, top
# Working serially with `foreach`

```{r eval=FALSE, echo=TRUE}
library(foreach)
foreach (i=1:3) %do% sqrt(i)
```
`foreach` creates an object & `%do%` operates on the code (here just one statement,
but it can be multiple lines between braces, as with a for loop) and the `foreach` object.

---
class: left, top
# `foreach`  &  `doParallel`

Foreach works with variety of backends to distribute computation - `doParallel`, which allows snow- and
multicore-style parallelism   

Switching the above loop to parallel just requires registering a backend and using `%dopar%` rather than `%do%`:
```{r}
library(parallel)
library(iterators)
```
```{r echo=TRUE}
library(doParallel)
registerDoParallel(3)  # use multicore-style forking
foreach (i=1:3) %dopar% sqrt(i)
stopImplicitCluster()
```

---
class: left, top
# Combining results

Default is to return a list, but there are numerous ways to combine results:
```{r echo=TRUE}
foreach (i=1:3, .combine=c) %do% sqrt(i)
foreach (i=1:3, .combine=cbind) %do% sqrt(i)
foreach (i=1:3, .combine="+") %do% sqrt(i)
foreach (i=1:3, .multicombine=TRUE, .combine="sum") %do% sqrt(i)
```

---
class: left, top
# Combining results

Most of these are self explanatory.

`multicombine` is worth mentioning: by default, `foreach` will combine
each new item into the final result one-at-a-time.

If `.multicombine=TRUE`, then you are saying that you're passing a function
which will do the right thing even if foreach gives it a whole wack of new results as a list or vector -
*e.g.*, a whole chunk at a time.

---
class: left, top
# Nesting `foreach`

There's one more operator: `%:%`.  This lets you compose or nest foreach objects:
```{r echo=TRUE}
foreach (i=1:3, .combine="c") %:% 
  foreach (j=1:3, .combine="c") %do% {
    i*j
  }
```
---
class: left, top
# Yet to cover

- Excercises
- `profvis` package
- Few slides on `Rcpp`
- Intro to Cloud services (broad overview & re-direct to UW resources)
